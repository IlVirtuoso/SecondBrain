{
	"nodes":[
		{"id":"b8097a7d49d99bc0","type":"text","text":"## Unbiased sample mean\nconsider a sample of indipendent observation with p sample of a random variable X, with mean $\\micro$ and variance $\\sigma^2$, the expected value $E[X]=\\micro$ , while $\\hat{\\micro}= \\bar{X}$.\nfrom the definition of $\\hat{u}$  we can observe that $\\bar{X}$ is an unbiased estimator for $\\micro$ \nand plugging $VAR[\\hat{u}] = E[(\\hat{u}-E[\\hat{u})^2]$ obtaining $VAR = \\frac{\\sigma^2}{p}$ we can conclude that $\\bar{X}$ is a good estimator of $\\micro$ ","x":-620,"y":-705,"width":640,"height":240,"color":"3"},
		{"id":"b9b70fd67905fa0b","type":"text","text":"## Variance of sample variance\nlet $X_j$ be the sub sample of the observations without the j-th one.  Compute:\n- Sample mean: $\\bar{X}_j = \\frac{1}{p-1} \\sum^{p}_{i=1;i \\neq j}x_i$ \n- Sample variance: $s^2_j = \\frac{1}{p-2} \\sum^{p}_{i=1;i \\neq j}(x_i-\\bar{X}_j)^2 = \\frac{1}{p-2} \\sum^{p}_{i=1;i \\neq j} x_i^2 \\frac{p-1}{p-2}\\bar{X}_j^2$.\n\nIt's easy to show that : $\\bar{X} = \\frac{1}{p} \\sum^{p}_{j=1}\\bar{X}_j$ , not the same could be said for \n$\\hat{s}^2=\\frac{1}{p}\\sum^{p}_{j=1}\\hat{s}_j^2$ \n\nNow we can use those results to get information about the distribution of the sample variance as a function of the sample size.\ndefine the following auxiliary variable: $Z_j=p \\hat{s}^2 - (p-1)\\hat{s}^2$  we know by definition that $E[\\hat{s}^2] = E[s_j^2]= E[\\sigma^2]$ and so $E[Z_j]=\\sigma^2$ . Consider a sample composed by those variables, then \n$\\bar{Z}=\\frac{1}{p} \\sum^{p}_{j=1}Z_j$  and $\\hat{s}^2= \\frac{1}{p-1}\\sum^{p}_{j=1}(Z_j-\\bar{Z})^2$. It can be shown that $\\Upsilon = \\frac{(\\bar{Z}-\\sigma^2)}{\\sqrt{\\hat{s}^2_Z/p}}$  is distributed as a t-student with p grade of freedom (a distribution with expected value to be equal to 0 and variance that decreases as the grade of freedom increases). We can then conclude that $\\bar{Z}$ is a good estimator for $\\sigma^2$ and $\\hat{s}^2$ as well.   ","x":370,"y":-1580,"width":620,"height":540,"color":"1"},
		{"id":"da8e2a386b52f05b","type":"text","text":"## Chebichev inequality\nChebyshevâ€™s inequality relates to the number of points that lie within k standard deviation of the mean. \nA) Points farthest from the mean contribute most to s and $\\hat{s}^2$  \n\nB) define the set $S_k=\\{x_i|\\bar{X}-k \\hat{s} < x_i < \\bar{X} + k \\hat{s}\\}$ from the sample variance definition we have \n\n$$\n(p-1) \\hat{s}^2= \\sum_{x_i \\in S_k}(x_i - \\bar{X})^2 + \\sum_{x_i \\in \\bar{S}_k} (x_i - \\bar{X})^2\n$$\nWhere $\\bar{S}_k$ is the complement of $S_k$ , ignoring the contribution to $\\hat{s}^2$ from all the points close to mean we get $(p-1)\\hat{s}^2 \\geq \\sum_{x_i \\in \\bar{S}_k}(x_i-\\bar{X})^2 \\geq \\sum_{x_i \\in \\bar{S}_k}(k \\bar{s})^2 = |\\bar{S}_k|(k \\hat{s})^2$. using $q_k=|S_k|/p$ ad the proportion of $x_i$ within $\\underline{+} k \\hat{s}$ of $\\bar{X}$  and removing $\\hat{s}^2$ we get chebichev inequality\n$$\nq_k \\geq 1- \\frac{(p-1)/p}{k^2} \\geq 1 - \\frac{1}{k^2}\n$$\n- For any sample at least 75% of the points lie within $\\underline{+}2 \\hat{s}^2$ of $\\bar{X}$ \n- For k=2 is very conservative (tipically the 95% lie in $\\underline{+}2 \\hat{s}^2$ of $\\bar{X}$) \n- $\\bar{X} \\underline{+}2 \\hat{s}^2$  defines the effective width of a sample (and so is 4s) , most but not all points lie in this interval (except outliers) ","x":327,"y":-2480,"width":707,"height":561,"color":"2"},
		{"id":"5b46f2108d6d219d","type":"text","text":"## Unbiased sample variance \nsimilar to sample mean  define $\\hat{s}^2=\\frac{1}{K}\\sum^{i=1}_{p}(x_i-\\hat{\\micro})^2$ , we want to show when $E[\\hat{s}^2]=\\sigma^2$ hold for which value of K. With some passages we can say that if we choose a value of K=(p-1) we can easily derive $E[\\hat{s}^2] =  \\frac{(p-1)p}{pK}* \\sigma^2= \\sigma^2$. We can then conclude that $s^2$ is an unbiased estimator of $\\sigma^2$ , computed with $\\frac{1}{p-1} \\sum^{i=1}_{p}(x_i-\\bar{X})^2$ \n","x":360,"y":-727,"width":640,"height":285,"color":"3"},
		{"id":"a5547afd75f33040","type":"text","text":"## Bias of an estimator\nthe bias (of a parameter) is the difference between the expected value of the estimator and the real value of the parameter.\n\na good estimator $\\hat{\\psi}$  of a parameter $\\psi$  if:\n- the distribution of $\\hat{\\psi}$ is centered over $\\psi$ \n- the distribution of $\\hat{\\psi}$ is narrow enough so that $|\\hat{\\psi} - \\psi|$  falls in the precision limits\n- information and distribution of $\\hat{\\psi}$ can be obtained from sample values and specifically from the sample size.","x":380,"y":-119,"width":600,"height":329,"color":"1"},
		{"id":"66528b1a8f469680","type":"text","text":"## Discrete data histogram\ngiven a discrete data sample multiset $S=\\{x_1,x_2,...,x_p\\}$ with possible value $\\chi$ , the relative frequency is $\\hat{f}(x)= \\frac{\\# \\ of \\ x_i \\in S \\ with \\ x_i=x}{p}$ , a discrete data histogram is a graphical display of $\\hat{f}(x)$ versus x.\n\n- Discrete data histogram mean $\\bar{X} = \\sum_{x in \\chi}x \\hat{f}(x)$ \n- Ddh std deviation $s=\\sqrt{(\\sum_{x}x^2\\hat{f}(x)) - \\bar{X}^2}$ \n- Ddh variance $s^2$ \n\nsample mean and std deviation of ddh is mathematically equivalent to the sample version, if the frequencies have already been computed then $\\bar{X}$ and s should be computed using ddh equations.\n\nIf the cumulative version of a distribution is preferred, use \n$\\hat{F}(x) = \\frac{\\# \\ of \\ x_i \\in S \\ with \\ x_i\\leq x}{p}$ , useful for quantiles.","x":-1540,"y":40,"width":500,"height":500,"color":"1"},
		{"id":"447b443e95e20600","type":"text","text":"## Continuos data histogram\nreal valued sample $S={x_1,x_2,...,x_p}$ , data value are generally distinct, lower and upper bounds a,b $a \\leq x_i < b$. Defines interval of possible values for random var X $\\chi=[a,b)=\\{x|a \\leq x < b\\}$.\n\n","x":-2520,"y":200,"width":500,"height":180,"color":"1"},
		{"id":"24b290c75b4b0cdd","type":"text","text":"## Binning\npartition the interval $\\chi= [a,b)$ into k equal width intervals. The bins are \n$B_0=[a,a+\\delta)$ , $B_1=[a+\\delta,a+2\\delta),...$ The width of each bin is \n$\\delta=(b-a)/k$ . For each $x \\in [a,b)$ there is a unique $B_j$ with $x \\in B_j$ \n- Estimated frequency of data values in $B_j$ is $\\phi_j=\\frac{\\# \\ x_i \\in S \\ for \\ which \\ x_i \\in B_j}{p}$ \n- Estimated density of random variable X $\\hat{f}(x)= \\hat{f}_j(x)=\\frac{\\phi_j}{\\delta}$ if $x \\in B_j$ \n- Density: frequency normalized via division by $\\delta$ \n- Frequency: density multiplied by $\\delta$ \n\nRaising or lowering $\\delta$ create smooth or noisy histograms\n","x":-3400,"y":-220,"width":620,"height":340,"color":"1"},
		{"id":"a1472237a7ad36c7","type":"text","text":"## Relative frequency\nDefine $p_j$ as the relative frequency of points in bin $B_j$ \nDefine the bin midpoints $m_j=a+(j+\\frac{1}{2})\\delta$.\n\nThen $p_j=\\delta \\hat{f}(m_j)$ ","x":-3420,"y":190,"width":620,"height":200,"color":"3"},
		{"id":"f0fdfaa73cf5f496","type":"text","text":"## Sample mean and standard statistics\n- Sample mean: $\\bar{X} = \\frac{1}{p} \\sum^{i=1}_{p}x_i$\n- Sample variance: $\\hat{s}^2 =\\frac{1}{p-1}\\sum^{i=1}_{p}(x_i-\\bar{X})^2$    \n- Sample standard deviation: $\\hat{s} = \\sqrt{\\hat{s}^2}$\n- Coefficient of variation: $\\frac{\\hat{s}}{\\bar{X}}$ \n\nmean: measure of central tendency\nvariance , deviation: measure of dispersion about the mean\n","x":-550,"y":-119,"width":500,"height":329,"color":"1"},
		{"id":"874af0399b58c0ef","type":"text","text":"## Time averaged statistics\nx(t) is the sample path of a stochastic process ($0<t< \\tau$) \n- Sample path mean $\\bar{X} = \\frac{1}{\\tau} \\int^{\\tau}_{0}x(t) dt$ \n- Sample path variance $s^2=\\frac{1}{\\tau} \\int^{\\tau}_{0}(x(t)- \\bar{X})^2 dt$ \n- Sample path std deviation $s=\\sqrt{s^2}$\n- One pass equation for variance $s^2=(\\frac{1}{\\tau} \\int^{\\tau}_{0}x^2(t)dt) - \\bar{X}^2$ \n\n\nin discrete event simulation sample path is piecewise, and so integrals reduce to summations.","x":-550,"y":420,"width":500,"height":320,"color":"1"},
		{"id":"8f184a440899379d","type":"text","text":"## Preliminary result\n\nif $X_1,X_2,...,X_p$ is an iid sequence of Normal Random Variables with common $\\micro$ and $\\sigma$ and $\\bar{X}$ is the mean of the sequence, then $\\bar{X}$ is a $Normal(\\micro,\\sigma,\\sqrt{p})$ Random Variable independently of the value of p.\n\n","x":-595,"y":880,"width":590,"height":200,"color":"2"},
		{"id":"d3e8350e15bdf842","type":"text","text":"## Random interval \ndefine $Z = \\frac{\\bar{X} - \\micro}{\\sigma/\\sqrt{p}}$ , Z has a standard normal distribution with $\\micro$ = 0 and $\\sigma$ = 1.\ngiven a parameter $\\alpha \\in (0.0,1.0)$ exists a positive number $z_{\\alpha/2}$ such that \n$$\nPr(-z_{\\alpha/2} \\leq Z \\leq z_{\\alpha/2}) = 1- \\alpha\n$$\nor replacing with the definition of Z\n$$\nPr(\\bar{X}- \\frac{z_{\\alpha/2} \\sigma}{\\sqrt{p}} \\leq \\micro \\leq \\bar{X}+ \\frac{z_{\\alpha/2} \\sigma}{\\sqrt{p}})\n$$\nNB: $\\bar{X}$ is a RV, meanwhile $\\micro$ is constant (=0)","x":-607,"y":1240,"width":615,"height":340,"color":"1"},
		{"id":"570f4d524f08863a","type":"text","text":"## Central limit theorem \nfollowing the definition of the sequence. $\\bar{X}$ approaches a $Normal(\\micro,\\sigma,\\sqrt{p})$ RV as $p \\rightarrow \\infty$ ","x":-1520,"y":900,"width":520,"height":160,"color":"1"},
		{"id":"5050e25cca4c5dc4","type":"text","text":"## Histogram mean and standard deviation\n$\\bar{X} = \\int^{b}_{a}x \\hat{f}(x) dx$        $s=\\sqrt{\\int^{b}_{a}(x-\\bar{X})^2\\hat{f}(x)dx}$ \nX and s can be evaluated by summation \n$\\bar{X} = \\sum^{k-1}_{j=0}m_jp_j$           $s=\\sqrt{(\\sum^{k-1}_{j=0}m_j^2p_j)-\\bar{X}^2 + \\frac{\\delta^2}{12}}$  (some choose to ignore the delta fraction). This is possible due to derivation of equation from those integrals \n$\\int^{b}_{a} x \\hat{f}(x)dx$  and $\\int^{b}_{a}x^2\\hat{f}(x)dx$  due to the fact that $\\hat{f}(m_j)= \\frac{p_j}{\\delta}$ ","x":-3440,"y":540,"width":640,"height":260,"color":"2"},
		{"id":"eeead7274728805b","type":"text","text":"## Properties of sample mean histogram\n- the histogram mean is approximately $\\micro$\n- the histogram std deviation is approx $\\sigma/\\sqrt{p}$ \n-  if p is sufficiently large the histogram density approximate the $Normal(\\micro,\\sigma,\\sqrt{p})$ pdf.\n\nI.E whatever is the distribution of the generated var (exponential, neg exp ecc...) if we put the values on an hist the density always approximate a normal RV","x":-2580,"y":835,"width":620,"height":291,"color":"2"},
		{"id":"fe87708ca97bae8d","type":"text","text":"## Standardize sample mean distribution \nwe can standardize the sample means $\\bar{x}_1,\\bar{x}_2,...$ by substracting $\\micro$ and divide by $\\sigma/\\sqrt{p}$ to form standardize sample means $z_1,z_2,...$ and so \n$$\nz_j=\\frac{\\bar{x}_j- \\micro}{\\sigma/\\sqrt{p}} \n$$\n\n","x":-2580,"y":1260,"width":620,"height":220,"color":"1"},
		{"id":"e08606f94165289a","type":"text","text":"## T statistic distribution\nreplace population std dev $\\sigma$ with sample std dev $\\hat{s}_j$ in $z_j$ equation. recall that:\n- each $\\bar{x}_j$ is a point estimate of $\\micro$ \n- each $\\hat{s}_j^2$ is a point estimate of $\\sigma$^2 (and so for sj)\n$$\nt_j=\\frac{\\bar{x_j}-\\micro}{\\hat{s}_j/\\sqrt{p}} \n$$\n","x":-3500,"y":1235,"width":640,"height":270,"color":"1"},
		{"id":"6434d3f16de1b1d0","type":"text","text":"## Interval estimation\nif $x_1,x_2,...,x_p$ is an independent random sample from a source of data with unknown $\\micro$ , if $\\bar{x}$ and $\\hat{s}$  are the mean and std dev of this sample and if p is large it is approximately true that $t = \\frac{\\bar{x}-\\micro}{\\hat{s}/\\sqrt{p}}$ is a Student(p-1) random variate.\n\n- this theorem provide a justification for estimating an interval that is likely to contain the mean $\\micro$ \n- as $p \\rightarrow \\infty$ the Student(p-1) is indistinguishable from Normal(0,1)","x":-4520,"y":1233,"width":640,"height":275,"color":"1"},
		{"id":"ee5e28c69febb818","type":"text","text":"## Properties of standardized sample mean histogram\n- the histogram mean is approximately 0\n- the histogram std dev is approximately 1\n- if p is sufficiently large the histogram density approximates the Normal(0,1) pdf","x":-2590,"y":1600,"width":640,"height":200,"color":"1"},
		{"id":"b1d03707f454789d","type":"text","text":"## Properties of t statistic histogram \n- if p>2 the histogram mean is approx 0\n- if p>3 the histogram std dev is approx $\\sqrt{\\frac{p-1}{p-3}}$ \n- if p is sufficiently large the histogram density approx a Student(p-1) RV","x":-3500,"y":1600,"width":640,"height":200,"color":"1"},
		{"id":"161bdcf02acafb39","type":"text","text":"## Confidence parameter\nSuppose: T is a Student(p-1) RV, $\\alpha$ is a confidence parameter $\\in (0.0,1.0)$ then exists a corresponding positive real number $t^*$ also denoted as $t_{(p-1,\\alpha/2)}$ \n$$\nPr(-t^*\\leq T \\leq t^*) = 1- \\alpha\n$$\n\nsuppose $\\micro$ is unknown. Since $t \\approx Student(p-1)$ then\n$$\n-t^* \\leq \\frac{\\bar{x}-\\micro}{\\hat{s}/\\sqrt{p}} \\leq t^*\n$$\nwill be approximately true with probability $1-\\alpha$ . So with probability $1-\\alpha$ :\n$$\n\\bar{x}- \\frac{t^*\\hat{s}}{\\sqrt{p}} \\leq \\micro \\leq \\bar{x}+\\frac{t^*\\hat{s}}{\\sqrt{p}}\n$$\nNB $\\micro = 0$ \nTheorem: if \n- $x_1,x_2,...,x_p$ is an independent random sample froma  source of data with unknown $\\micro$ \n- $\\bar{x}$ and $\\hat{s}$ are the sample mean and sample std deviation\n- p is large \nthen given a confidence parameter $\\alpha \\in (0.0,1.0)$ there exist an associated positive real number $t^*$ such that  $Pr(\\bar{x}- \\frac{t^*\\hat{s}}{\\sqrt{p}} \\leq \\micro \\leq \\bar{x}+\\frac{t^*\\hat{s}}{\\sqrt{p}}) \\approx 1-\\alpha$  ","x":-4520,"y":1640,"width":640,"height":620,"color":"1"},
		{"id":"87646d679052ae11","type":"text","text":"## Stopping rule\nthe asymptotic value of $t^*$ is $t^*_\\infty= \\lim_{p \\rightarrow \\infty}t-Student(p-1,1-\\alpha/2) = Normal(0.0,1.0,1-\\alpha/2)$ , unless $\\alpha$ is very close to 0.0 if p>40 we can use $p^*_\\infty$ \n\nGiven a reasonable guess for $\\hat{s}$ and a user specified half-width w, how much sample we must collect?\nusing $w = \\frac{t^*\\hat{s}}{\\sqrt{p}}$ we can resolve for $p=\\lfloor (\\frac{t_\\infty^*\\hat{s}}{w})^2 \\rfloor$   ","x":-3220,"y":2493,"width":520,"height":295,"color":"1"},
		{"id":"c223e4cbc66e7c5c","type":"text","text":"## Sequential Stopping rule\nif a reasonable guess of $\\hat{s}$ is not available, w can be specified as a proportion of $\\hat{s}$ eliminating it from the previous equation\n\nfor example if w is 10% of $\\hat{s}$  and 95% of confidence is desired, p=384 should be used to estimate $\\micro$ to within $\\underline{+}w$ , after having generated an initial sample of this size, check whether the required precision has been achieved, if not use the sample derived from this sample to predict a new larger sample size.\n\nw can be used as a precision parameter, assuming an initial value of p>40:\n- Compute a first value for $\\bar{x}$ and $\\hat{s}$ and check $\\frac{w}{\\bar{x}} = \\frac{t^*\\hat{s}}{\\bar{x}\\sqrt{p}} < \\epsilon$ \n- If the inequality is not satisfied the value of p can be guessed to be at least $p=\\lfloor (\\frac{t_\\infty^*\\hat{s}}{\\bar{x}\\epsilon})^2 \\rfloor$\n- Repeat until the desired precision is not reached","x":-3270,"y":2920,"width":620,"height":440,"color":"1"},
		{"id":"d6475e29ece70fbe","type":"text","text":"## Confidence interval \ngiven a random sample $x_1,x_2,...,x_p$ if we can calculate $\\bar{x}$ and $\\hat{s}$ , the interval with endpoints $\\bar{x} \\underline{+} \\frac{t^*\\hat{s}}{\\sqrt{p}}$ is a random range of values that contains the true parameter $\\micro$ with $(1-\\alpha)\\%$  confidence.\n\nFor calculate an interval estimate for the unknown $\\micro$ :\n- Pick a level of confidence $1-\\alpha$ (tipically $\\alpha=0.05$) \n- Calculate $\\bar{x}$ and $\\hat{s}$ \n- Calculate critical value $t^*=t-Student(p-1,1-\\alpha/2)$ \n- Calculate interval endpoints $\\bar{x} \\underline{+} \\frac{t^*\\hat{s}}{\\sqrt{p}}$ ","x":-4520,"y":2440,"width":640,"height":400,"color":"1"}
	],
	"edges":[
		{"id":"6615deba6dfb0a8c","fromNode":"f0fdfaa73cf5f496","fromSide":"top","toNode":"b8097a7d49d99bc0","toSide":"bottom"},
		{"id":"dcac2f1807ce0d70","fromNode":"a5547afd75f33040","fromSide":"top","toNode":"b8097a7d49d99bc0","toSide":"bottom"},
		{"id":"4f51e671c9300931","fromNode":"f0fdfaa73cf5f496","fromSide":"top","toNode":"5b46f2108d6d219d","toSide":"bottom"},
		{"id":"391f292b182abd4e","fromNode":"a5547afd75f33040","fromSide":"top","toNode":"5b46f2108d6d219d","toSide":"bottom"},
		{"id":"2ac9cf9e1d3ca3fe","fromNode":"b8097a7d49d99bc0","fromSide":"right","toNode":"5b46f2108d6d219d","toSide":"left"},
		{"id":"d381540f50927641","fromNode":"5b46f2108d6d219d","fromSide":"top","toNode":"b9b70fd67905fa0b","toSide":"bottom"},
		{"id":"0164dd90c755bfb8","fromNode":"b9b70fd67905fa0b","fromSide":"top","toNode":"da8e2a386b52f05b","toSide":"bottom"},
		{"id":"dbc5067ff74d8715","fromNode":"f0fdfaa73cf5f496","fromSide":"bottom","toNode":"874af0399b58c0ef","toSide":"top"},
		{"id":"97d98a3f71a09186","fromNode":"f0fdfaa73cf5f496","fromSide":"left","toNode":"66528b1a8f469680","toSide":"right"},
		{"id":"2cc581ec7d53e00a","fromNode":"874af0399b58c0ef","fromSide":"left","toNode":"66528b1a8f469680","toSide":"right"},
		{"id":"d1c47cd80e5ac4a5","fromNode":"66528b1a8f469680","fromSide":"left","toNode":"447b443e95e20600","toSide":"right"},
		{"id":"77ac01b23103a240","fromNode":"447b443e95e20600","fromSide":"left","toNode":"24b290c75b4b0cdd","toSide":"right"},
		{"id":"a9ad27e1dbf8e96c","fromNode":"447b443e95e20600","fromSide":"left","toNode":"a1472237a7ad36c7","toSide":"right"},
		{"id":"b8149ffbc21e8c03","fromNode":"874af0399b58c0ef","fromSide":"bottom","toNode":"8f184a440899379d","toSide":"top"},
		{"id":"f383f66633ecfb95","fromNode":"8f184a440899379d","fromSide":"bottom","toNode":"d3e8350e15bdf842","toSide":"top"},
		{"id":"75de6f6589a54a17","fromNode":"570f4d524f08863a","fromSide":"left","toNode":"eeead7274728805b","toSide":"right"},
		{"id":"0e4b8b38e38c6228","fromNode":"8f184a440899379d","fromSide":"left","toNode":"570f4d524f08863a","toSide":"right"},
		{"id":"95b5e806cecb15b0","fromNode":"447b443e95e20600","fromSide":"bottom","toNode":"eeead7274728805b","toSide":"top"},
		{"id":"002e7f0c55a44406","fromNode":"5050e25cca4c5dc4","fromSide":"right","toNode":"eeead7274728805b","toSide":"top"},
		{"id":"3d912fbb7f1f754b","fromNode":"447b443e95e20600","fromSide":"left","toNode":"5050e25cca4c5dc4","toSide":"right"},
		{"id":"73166e59a14d5e71","fromNode":"eeead7274728805b","fromSide":"bottom","toNode":"fe87708ca97bae8d","toSide":"top"},
		{"id":"652eeb7677945e0c","fromNode":"fe87708ca97bae8d","fromSide":"bottom","toNode":"ee5e28c69febb818","toSide":"top"},
		{"id":"de208e7d07cca7d3","fromNode":"fe87708ca97bae8d","fromSide":"left","toNode":"e08606f94165289a","toSide":"right"},
		{"id":"fefacb07c325139b","fromNode":"e08606f94165289a","fromSide":"bottom","toNode":"b1d03707f454789d","toSide":"top"},
		{"id":"d3d798492c4f790b","fromNode":"e08606f94165289a","fromSide":"left","toNode":"6434d3f16de1b1d0","toSide":"right"},
		{"id":"3381d5c8cea7c679","fromNode":"6434d3f16de1b1d0","fromSide":"bottom","toNode":"161bdcf02acafb39","toSide":"top"},
		{"id":"8acfc62feadc5901","fromNode":"161bdcf02acafb39","fromSide":"bottom","toNode":"d6475e29ece70fbe","toSide":"top"},
		{"id":"59458fb25005b6ae","fromNode":"d6475e29ece70fbe","fromSide":"right","toNode":"87646d679052ae11","toSide":"left"},
		{"id":"54a8d18be4119af9","fromNode":"87646d679052ae11","fromSide":"bottom","toNode":"c223e4cbc66e7c5c","toSide":"top"}
	]
}