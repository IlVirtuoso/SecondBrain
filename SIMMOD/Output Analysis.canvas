{
	"nodes":[
		{"id":"1fb92f5716a05bcc","type":"text","text":"## Terminating conditions \ndepending on statistics chosen they can be expressed in term of simulation time or number of processed events. \n\nin the case of an SSQ we want to estimate: \n- the average number of customers in the system up to time T (1)\n- the average waiting time experienced by the first N customers (2)\n\nFormally \n1) state variable $n(\\cdot)$ is known as stochastic process. typical objective estimate time-averaged transient statistic $\\bar{n}(T)=\\frac{1}{T} \\int^{T}_{0}n(t) dt$ .\n2) $W_i$ also is a stochastic process. typical objective estimate sample-averaged transient statistic $\\bar{W}(N)= \\frac{1}{N}\\sum^{N}_{i=1}W_i$ \n","x":-280,"y":280,"width":580,"height":420,"color":"2"},
		{"id":"892b702f101ad196","type":"text","text":"## Indipendent repications\nthe samples collected in a set of replication are not independent (output are correlated), so is possible to calculate mean and variance but they are not independent, so is necessary a system to overcome this problem.\n\nThus the simulation can be repeated varying only the seed of the generator, the totality of the replication is an ensemble or sample, the seed must be chosen so that there is no overlapping between simulations, tipically the final state of the previous simulation is used as seed for the new one.","x":560,"y":-210,"width":580,"height":340,"color":"1"},
		{"id":"6538a838c6494722","type":"text","text":"[[Interval estimation.canvas|Interval estimation]]","x":1580,"y":-520,"width":200,"height":60,"color":"2"},
		{"id":"d227042735bc8cac","type":"text","text":"## Waiting time of the K-th customer (jackknifing)\n$W_k^{[i]} :i =1,2,...p$ = sample of measurement of waiting time of the k-th customer (only).\n\n$\\bar{W}_k=\\frac{1}{p}\\sum^{p}_{i=1}W_k^{[i]}$  ,     $\\hat{s}^2_{W_k}=\\frac{1}{p-1}\\sum^{p}_{i=1}(W_k^{[i]} - \\bar{W}_k)^2$ \n- we obtain confidence interval for $\\micro = E[W_k]$  --> $Pr\\{\\bar{W}_k-t_{(p-1,\\frac{\\alpha}{2})} \\frac{\\hat{s}_{W_k}}{\\sqrt{p}} \\leq \\micro \\leq \\bar{W}_k+t_{(p-1,\\frac{\\alpha}{2})} \\frac{\\hat{s}_{W_k}}{\\sqrt{p}}\\} \\approx (1-\\alpha)$ \n- we compute the point estimate of the variance \n$\\hat{s}^2 = \\frac{1}{p-1}\\sum^{p}_{i=1}(W_k^{[i]}-\\bar{W}_k)^2$     $E[\\hat{s}^2]=\\sigma^2$ \n- subdivide the original sample into p sub sample of size p-1 items $(W_k)_j$ \n$\\bar{W_k}_j=\\frac{1}{p-1}\\sum^{p}_{i=1;i \\neq j}W_k^{[i]}$        $\\hat{s}^2=\\frac{1}{p-2}\\sum^{p}_{i=1;i \\neq j}(W_k^{[i]})^2 - \\frac{p-1}{p-2}\\bar{(W_k)_j^2}$  \n\ndefine $Z_j=p \\hat{s}^2- (p-1)\\hat{s}_j^2$  knowing $E[Z_j]=\\sigma^2$ we can form a new sample of $Z_i$ \n$\\bar{Z}=\\frac{1}{p}Z_j$     $\\hat{s}_Z^2=\\frac{1}{p-1}\\sum^{p}_{j=1}(Z_j-\\bar{Z})^2$ \n\nwe can then define $\\Upsilon = \\frac{(\\bar{Z}-\\sigma^2)}{s_Z/\\sqrt{p}}$ , and show that is a RV distributed as a t-student with p-1 grade of freedom  and then obtain the confidence interval for $\\sigma^2$ \n$$\nPr\\{\\bar{Z} - t_{(p-1,\\frac{\\alpha}{2})} \\frac{\\hat{s}_Z}{\\sqrt{p}} \\leq \\sigma^2 \\leq \\bar{Z} + t_{(p-1,\\frac{\\alpha}{2})} \\frac{\\hat{s}_Z}{\\sqrt{p}} \\} \\approx (1-\\alpha)\n$$\n\nthis method is know as jackknifing\n\n","x":2261,"y":-330,"width":700,"height":580,"color":"1"},
		{"id":"e7c5ed1f4adca3b7","type":"text","text":"## Replication and interval estimation\nsuppose the simulation is replicated p times, each time generating a state time history $x_i(t)$ -> $\\bar{x_i}(T)=\\int^{T}_{0}x_i(t) dt$ where i is the replication index.\n\n- Each data point $\\bar{x}_i(T)$ is a indipendent observation of the RV $\\bar{X}(T)$ \n- if p is large enough, pdf of $\\bar{X}(T)$ can be estimated from histogram of $\\bar{x}_i(T)$  \n\nSpecifically if we want $E[\\bar{X}(T)]$ :\n- a point estimate is available as a sample average $\\hat{\\micro}=\\frac{1}{p}\\sum^{p}_{i=1}\\bar{x}_i(T)$ \n- an interval estimate for $E[\\bar{X}(T)]$ can be calculated: Using the [[Interval estimation.canvas|Interval estimation]] technique. This requires sample mean and std deviation of $\\bar{x}_i(T)$ , this can be used to estimate $\\bar{n}(T)$ and $\\bar{W}(N)$ \n\nlet $n(T)$ be the number of customers in the system at time T, suppose we want $E[n(T)]$ \n- n(T) is a RV and the result of the simulation at time T is an instance of RV\n- in general the distribution of n(T) is far from being an approx Normal \n- repeating the simulation several times produces data that are hard to analyze with standard confidence interval techniques.\n- subdivide the sample of size p into p/K subsample is a good way to compute the average results (with proper confidence intervals) that can be used as a set of measure for other computations.","x":1330,"y":-360,"width":700,"height":640,"color":"1"},
		{"id":"1e27b80b5d92dc0f","type":"text","text":"## Probability of lying in a fixed interval \n\nAssume a sample of p observation of an RV $Y_i \\in Y,i=1...p$ coming from p independent observation, we want to estimate the probability that Y takes values within interval I\n$\\psi = Prob\\{Y \\in I\\}$ , let X be an RV derived from Y $X_i= \\begin{cases} 1 \\ \\ if \\ Y_i \\in i \\\\ 0\\end{cases}$   then $E[X]=E[X_i]=\\psi$ \n\ndefine m to be the number of sample components $Y_i$ that belong to I -> $m = \\sum^{p}_{i=1}X_i$ \n\nThe unbiased estimate of this prob is $\\psi = \\frac{1}{p}\\sum^{p}_{i=1}X_i=\\frac{m}{p}$ where $E[\\psi]=\\frac{1}{p}\\sum^{p}_{i=1}E[X_i]=\\psi$ \nand $VAR[\\hat{\\psi}]=\\frac{\\psi(1-\\psi)}{p}$ \n\nm has binomial distribution with parameter $\\psi$  --> $Prob\\{m=k\\}= \\frac{p!}{k!(p-k)!} \\psi^k(1-\\psi)^{p-k}$ \nexpected value and variance of this RV are then \n$E[m]=p \\psi$  and $VAR[m]=p \\psi(1-\\psi)$ \n\nestimating $\\psi$ is equal to estimating the parameter of a binomial distribution when $\\psi$ is the probability of success $Prob\\{\\hat{\\psi}_L \\leq \\psi \\leq \\hat{\\psi}_U\\}$  where $\\hat{\\psi}_L$ and $\\hat{\\psi}_U$ can be obtained from resolving the following 2 equations \n\n$\\sum^{p}_{k=m}[\\frac{p!}{k!(p-k)!} \\psi_L^k(1-\\psi_L^k)^{p-k}] = \\frac{\\alpha}{2}$  for $\\psi_L$      \n$\\sum^{m-1}_{i=0}[\\frac{p!}{k!(p-k)!} \\psi_U^k(1-\\psi_U^k)^{p-k}] = \\frac{\\alpha}{2}$  for $\\psi_U$    \n\nwhen  p is large and both m and p-m are > 5  a simpler way to get the confidence interval for $\\psi$ is to use a normal distribution as an approximation of the binomial. So we can claim that $\\hat{\\psi}$ has a normal distribution with mean $\\psi$ and variance $\\frac{\\psi(1-\\psi)}{p}$  \n\nso with the following auxiliary variable $Z=\\frac{\\hat{\\psi} - \\psi}{\\sqrt{\\psi(1-\\psi}/p}$ , that has a standard normal distribution we can search $Pr(-z_{\\alpha/2} \\leq Z \\leq z_{\\alpha/2}) = 1-\\alpha$  and then the desired confidence interval becomes \n\n$$\nPr\\{\\frac{m}{p}-z_{\\alpha/2}\\sqrt{\\frac{m(p-m)}{p}} \\leq \\psi \\leq Pr\\{\\frac{m}{p}+z_{\\alpha/2}\\sqrt{\\frac{m(p-m)}{p}}\\}= 1-\\alpha\n$$\n","x":1325,"y":440,"width":710,"height":820,"color":"1"},
		{"id":"14aeb15597035112","type":"text","text":"## Finite horizon simulations  \nA finite-horizon discrete event simulation is one for which the simulated operational time is finite. they are also know as terminating simulations. Often the system state is assumed to be idle at the beginning and at the end of the simulation. The terminating condition can be specified by the close the door time.\n- Transient system statistics are produced by this simulation.\n- Initial conditions affect finite horizon statistics\n- No need to assume a static environment \n","x":-280,"y":-200,"width":580,"height":320,"color":"4"},
		{"id":"5f7d906f79d18ccf","type":"text","text":"## Infinite horizon simulations (steady state)\nSteady state statistics are produced by this type of simulations, in this case the initial conditions doesn't affect the statistics produced (system loses memory of the initial state). Thus the system environment is assumed to remain static.","x":-280,"y":-560,"width":580,"height":200,"color":"4"},
		{"id":"2f5ba528789510ce","type":"text","text":"## Terminating conditions\ndepending on the statistics chosen, the terminating conditions may assume different aspects. In general state variables $X(t)$ and $Y_i$ of a system are known formally as stochastic process. \n\nTipical objective are:\n-  time-averaged steady state statistics: $\\bar{x} = \\lim_{T \\rightarrow \\infty} \\frac{1}{T} \\int^{T}_{0}X(t)dt$ \n- sample-averaged steady state statistics: $\\bar{y}=\\lim_{N \\rightarrow \\infty} \\frac{1}{N} \\sum^{N}_{i=0}Y_i$ \nboth $\\bar{x}$ and $\\bar{y}$ are not random variables.\n\n","x":-280,"y":-1100,"width":580,"height":320,"color":"1"},
		{"id":"27dec0e4325100d4","type":"text","text":"## The challenge\nusing $\\{W_n:n =1,2,3,...\\}$ suppose we are interested in estimating the waiting time distribution in steady state: $\\lim_{n \\rightarrow \\infty} Prob\\{W_n \\leq x\\} = F(x)$.\nAs in the case of transitory state we are interested in calculate expected value and variance of this value. \n\nIn this case we don't need independent replications, because all RVs have all the same distribution. There are elements to take in consideration however:\n- The observation of the RV must be made after a set of initial observations\n- The observations made in a single simulation are in general highly correlated, and so many of the simple statistical method are ineffective since there is no independence\n\n","x":520,"y":-1130,"width":660,"height":380,"color":"1"},
		{"id":"ed464e34ce17dd95","type":"text","text":"## The length of the initial transient period \nThe initial condition must not:\n- affect the steady state behaviour \n- require a very long initial simulation before getting a stable condition. \n\nThe effect of those observation is that we must discard a long initial simulation.\nThe identification of the conditions is a difficult task that can only be addressed with preliminary simulations (pilot simulations).  The idea is to perform many simulation and collect statistics at different simulation times in order to construct samples that can be used to compute the probability distributions at these instant. So when compared they can be assumed as the steady state distribution of the process\n","x":1260,"y":-1580,"width":660,"height":360,"color":"1"},
		{"id":"cde292e24b57a8dd","type":"text","text":"## Comparing distributions \n- let $Y(t)$ be # of person in the system at time t and assume we made many observation $t_1,t_2,...,t_n$ --> $\\{Y_i(t_n),i=1...p\\} \\ \\ n=1,2,3...$ \n- let $m_k(t_n)$ be # of times $Y_i(t_n) = k$ --> $\\{m_k(t_n),k=1...N_{max}\\}$ \n- let $\\psi_k(t_n)$ be P of observing k person in the system at time $t_n$ \nThus we can compute $\\{\\psi_k(t_n)=\\frac{m_k(t_n)}{p}\\} \\ \\ k=1...N_{max}$ where at most $N_{max}$ customers have been observed in the system. \n\nThe initial transient period is defined as the value of index n where the difference between $t_n$ and $t_{n+1}$ becomes smaller than a certain threshold \n$$\nt_n:min_n\\{\\frac{\\sum^{N_{max}}_{k=1}[\\psi_k(t_n)-\\psi_k(t_{n+1}]^2}{N_{max}} < \\epsilon\\}\n$$\n","x":1260,"y":-2140,"width":660,"height":440,"color":"1"},
		{"id":"e4d876f751076206","type":"text","text":"## Comparing moments \ncomparing distribution need a big amount of data, a reasonable compromise is to compare moments instead of distributions. In this case we can compute \n$$\nt_n^{[h]}:min_n\\{|\\frac{\\sum^{p}_{i=1}[Y_i(t_n)]^h}{p} - \\frac{\\sum^{p}_{i=1}[Y_i(t_{n+1})]^h}{p}|<\\epsilon\\}\n$$\n\n","x":1260,"y":-2700,"width":660,"height":440,"color":"1"}
	],
	"edges":[
		{"id":"4de48dfd7459a729","fromNode":"14aeb15597035112","fromSide":"bottom","toNode":"1fb92f5716a05bcc","toSide":"top"},
		{"id":"777a78e500ea045f","fromNode":"14aeb15597035112","fromSide":"right","toNode":"892b702f101ad196","toSide":"left"},
		{"id":"c7b5897d525fcd95","fromNode":"892b702f101ad196","fromSide":"right","toNode":"e7c5ed1f4adca3b7","toSide":"left"},
		{"id":"61cf8ba3a20f08c9","fromNode":"6538a838c6494722","fromSide":"bottom","toNode":"e7c5ed1f4adca3b7","toSide":"top"},
		{"id":"a7a7706c6deb01f5","fromNode":"e7c5ed1f4adca3b7","fromSide":"right","toNode":"d227042735bc8cac","toSide":"left"},
		{"id":"396c0a7ade34703a","fromNode":"e7c5ed1f4adca3b7","fromSide":"bottom","toNode":"1e27b80b5d92dc0f","toSide":"top"},
		{"id":"6bead80d234a61ac","fromNode":"14aeb15597035112","fromSide":"top","toNode":"5f7d906f79d18ccf","toSide":"bottom"},
		{"id":"a1a6c50effac4ab0","fromNode":"5f7d906f79d18ccf","fromSide":"top","toNode":"2f5ba528789510ce","toSide":"bottom"},
		{"id":"34de1d39a5f4c351","fromNode":"2f5ba528789510ce","fromSide":"right","toNode":"27dec0e4325100d4","toSide":"left"},
		{"id":"42d52c3f791c44d9","fromNode":"27dec0e4325100d4","fromSide":"right","toNode":"ed464e34ce17dd95","toSide":"bottom"},
		{"id":"4e3dfc83f5207ff7","fromNode":"ed464e34ce17dd95","fromSide":"top","toNode":"cde292e24b57a8dd","toSide":"bottom"},
		{"id":"59c25a1bd8f3fa9f","fromNode":"cde292e24b57a8dd","fromSide":"top","toNode":"e4d876f751076206","toSide":"bottom"}
	]
}