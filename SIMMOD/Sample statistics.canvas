{
	"nodes":[
		{"id":"b8097a7d49d99bc0","type":"text","text":"## Unbiased sample mean\nconsider a sample of indipendent observation with p sample of a random variable X, with mean $\\micro$ and variance $\\sigma^2$, the expected value $E[X]=\\micro$ , while $\\hat{\\micro}= \\bar{X}$.\nfrom the definition of $\\hat{u}$  we can observe that $\\bar{X}$ is an unbiased estimator for $\\micro$ \nand plugging $VAR[\\hat{u}] = E[(\\hat{u}-E[\\hat{u})^2]$ obtaining $VAR = \\frac{\\sigma^2}{p}$ we can conclude that $\\bar{X}$ is a good estimator of $\\micro$ ","x":-620,"y":-705,"width":640,"height":240,"color":"3"},
		{"id":"b9b70fd67905fa0b","x":370,"y":-1580,"width":620,"height":540,"color":"1","type":"text","text":"## Variance of sample variance\nlet $X_j$ be the sub sample of the observations without the j-th one.  Compute:\n- Sample mean: $\\bar{X}_j = \\frac{1}{p-1} \\sum^{p}_{i=1;i \\neq j}x_i$ \n- Sample variance: $s^2_j = \\frac{1}{p-2} \\sum^{p}_{i=1;i \\neq j}(x_i-\\bar{X}_j)^2 = \\frac{1}{p-2} \\sum^{p}_{i=1;i \\neq j} x_i^2 \\frac{p-1}{p-2}\\bar{X}_j^2$.\n\nIt's easy to show that : $\\bar{X} = \\frac{1}{p} \\sum^{p}_{j=1}\\bar{X}_j$ , not the same could be said for \n$\\hat{s}^2=\\frac{1}{p}\\sum^{p}_{j=1}\\hat{s}_j^2$ \n\nNow we can use those results to get information about the distribution of the sample variance as a function of the sample size.\ndefine the following auxiliary variable: $Z_j=p \\hat{s}^2 - (p-1)\\hat{s}^2$  we know by definition that $E[\\hat{s}^2] = E[s_j^2]= E[\\sigma^2]$ and so $E[Z_j]=\\sigma^2$ . Consider a sample composed by those variables, then \n$\\bar{Z}=\\frac{1}{p} \\sum^{p}_{j=1}Z_j$  and $\\hat{s}^2= \\frac{1}{p-1}\\sum^{p}_{j=1}(Z_j-\\bar{Z})^2$. It can be shown that $\\Upsilon = \\frac{(\\bar{Z}-\\sigma^2)}{\\sqrt{\\hat{s}^2_Z/p}}$  is distributed as a t-student with p grade of freedom (a distribution with expected value to be equal to 0 and variance that decreases as the grade of freedom increases). We can then conclude that $\\bar{Z}$ is a good estimator for $\\sigma^2$ and $\\hat{s}^2$ as well.   "},
		{"id":"da8e2a386b52f05b","x":327,"y":-2280,"width":707,"height":361,"type":"text","text":"## Chebichev inequality\nChebyshevâ€™s inequality relates to the number of points that lie within k standard deviation of the mean. \nA) Points farthest from the mean contribute most to s and $\\hat{s}^2$  \n\nB) define the set $S_k=\\{x_i|\\bar{X}-k \\hat{s} < x_i < \\bar{X} + k \\hat{s}\\}$ from the sample variance definition we have \n\n$$\n(p-1) \\hat{s}^2= \\sum_{x_i \\in S_k}(x_i - \\bar{X})^2 + \\sum_{x_i \\in \\bar{S}_k} (x_i - \\bar{X})^2\n$$\nWhere $\\bar{S}_k$ is the complement of $S_k$ \n"},
		{"id":"5b46f2108d6d219d","type":"text","text":"## Unbiased sample variance \nsimilar to sample mean  define $\\hat{s}^2=\\frac{1}{K}\\sum^{i=1}_{p}(x_i-\\hat{\\micro})^2$ , we want to show when $E[\\hat{s}^2]=\\sigma^2$ hold for which value of K. With some passages we can say that if we choose a value of K=(p-1) we can easily derive $E[\\hat{s}^2] =  \\frac{(p-1)p}{pK}* \\sigma^2= \\sigma^2$. We can then conclude that $s^2$ is an unbiased estimator of $\\sigma^2$ , computed with $\\frac{1}{p-1} \\sum^{i=1}_{p}(x_i-\\bar{X})^2$ \n","x":360,"y":-727,"width":640,"height":285,"color":"3"},
		{"id":"a5547afd75f33040","type":"text","text":"## Bias of an estimator\nthe bias (of a parameter) is the difference between the expected value of the estimator and the real value of the parameter.\n\na good estimator $\\hat{\\psi}$  of a parameter $\\psi$  if:\n- the distribution of $\\hat{\\psi}$ is centered over $\\psi$ \n- the distribution of $\\hat{\\psi}$ is narrow enough so that $|\\hat{\\psi} - \\psi|$  falls in the precision limits\n- information and distribution of $\\hat{\\psi}$ can be obtained from sample values and specifically from the sample size.","x":380,"y":-119,"width":600,"height":329,"color":"1"},
		{"id":"f0fdfaa73cf5f496","type":"text","text":"## Sample mean and standard statistics\n- Sample mean: $\\bar{X} = \\frac{1}{p} \\sum^{i=1}_{p}x_i$\n- Sample variance: $\\hat{s}^2 =\\frac{1}{p-1}\\sum^{i=1}_{p}(x_i-\\bar{X})^2$    \n- Sample standard deviation: $\\hat{s} = \\sqrt{\\hat{s}^2}$\n- Coefficient of variation: $\\frac{\\hat{s}}{\\bar{X}}$ \n\nmean: measure of central tendency\nvariance , deviation: measure of dispersion about the mean\n","x":-550,"y":-119,"width":500,"height":329,"color":"1"}
	],
	"edges":[
		{"id":"6615deba6dfb0a8c","fromNode":"f0fdfaa73cf5f496","fromSide":"top","toNode":"b8097a7d49d99bc0","toSide":"bottom"},
		{"id":"dcac2f1807ce0d70","fromNode":"a5547afd75f33040","fromSide":"top","toNode":"b8097a7d49d99bc0","toSide":"bottom"},
		{"id":"4f51e671c9300931","fromNode":"f0fdfaa73cf5f496","fromSide":"top","toNode":"5b46f2108d6d219d","toSide":"bottom"},
		{"id":"391f292b182abd4e","fromNode":"a5547afd75f33040","fromSide":"top","toNode":"5b46f2108d6d219d","toSide":"bottom"},
		{"id":"2ac9cf9e1d3ca3fe","fromNode":"b8097a7d49d99bc0","fromSide":"right","toNode":"5b46f2108d6d219d","toSide":"left"},
		{"id":"d381540f50927641","fromNode":"5b46f2108d6d219d","fromSide":"top","toNode":"b9b70fd67905fa0b","toSide":"bottom"},
		{"id":"0164dd90c755bfb8","fromNode":"b9b70fd67905fa0b","fromSide":"top","toNode":"da8e2a386b52f05b","toSide":"bottom"}
	]
}